{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Multiplie Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2830    middle_age\n",
      "925          young\n",
      "3845    middle_age\n",
      "547          young\n",
      "2259    middle_age\n",
      "           ...    \n",
      "3444    middle_age\n",
      "466     middle_age\n",
      "3092    middle_age\n",
      "3772    middle_age\n",
      "860          young\n",
      "Name: Rings, Length: 2923, dtype: category\n",
      "Categories (3, object): ['young' < 'middle_age' < 'old']\n",
      "[0 2 0 ... 0 0 2]\n",
      "Training set: (2923, 11) (2923,)\n",
      "Testing set: (627, 11) (627,)\n",
      "Validation set: (627, 11) (627,)\n",
      "\n",
      "Training set:\n",
      "      Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
      "2830   0.525     0.430   0.135        0.8435          0.4325          0.1800   \n",
      "925    0.430     0.325   0.100        0.3645          0.1575          0.0825   \n",
      "3845   0.455     0.350   0.105        0.4160          0.1625          0.0970   \n",
      "547    0.205     0.155   0.045        0.0425          0.0170          0.0055   \n",
      "2259   0.590     0.465   0.160        1.1005          0.5060          0.2525   \n",
      "\n",
      "      Shell weight   Age  Sex_F  Sex_I  Sex_M  \n",
      "2830        0.1815  10.5    1.0    0.0    0.0  \n",
      "925         0.1050   8.5    0.0    1.0    0.0  \n",
      "3845        0.1450  12.5    0.0    0.0    1.0  \n",
      "547         0.0155   8.5    0.0    0.0    1.0  \n",
      "2259        0.2950  14.5    1.0    0.0    0.0  \n",
      "\n",
      "Testing set:\n",
      "      Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
      "3142   0.215     0.165   0.055        0.0590          0.0265          0.0125   \n",
      "752    0.605     0.460   0.170        1.1220          0.3470          0.3045   \n",
      "1188   0.690     0.540   0.155        1.4540          0.6240          0.3105   \n",
      "3740   0.650     0.500   0.170        1.4045          0.6940          0.3180   \n",
      "2586   0.555     0.425   0.140        0.9630          0.4400          0.2240   \n",
      "\n",
      "      Shell weight   Age  Sex_F  Sex_I  Sex_M  \n",
      "3142        0.0185   6.5    0.0    1.0    0.0  \n",
      "752         0.3150  14.5    1.0    0.0    0.0  \n",
      "1188        0.3900  10.5    1.0    0.0    0.0  \n",
      "3740        0.3235  12.5    1.0    0.0    0.0  \n",
      "2586        0.2400   8.5    1.0    0.0    0.0  \n",
      "\n",
      "Validation set:\n",
      "      Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
      "2148   0.415     0.310   0.090        0.3245          0.1305          0.0735   \n",
      "2303   0.580     0.455   0.120        0.9400          0.3990          0.2570   \n",
      "270    0.640     0.525   0.215        1.7790          0.4535          0.2855   \n",
      "3409   0.395     0.310   0.095        0.3130          0.1310          0.0720   \n",
      "3654   0.530     0.420   0.140        0.6765          0.2560          0.1855   \n",
      "\n",
      "      Shell weight   Age  Sex_F  Sex_I  Sex_M  \n",
      "2148         0.115   9.5    0.0    0.0    1.0  \n",
      "2303         0.265  12.5    1.0    0.0    0.0  \n",
      "270          0.550  23.5    1.0    0.0    0.0  \n",
      "3409         0.093   8.5    0.0    1.0    0.0  \n",
      "3654         0.208  10.5    0.0    1.0    0.0  \n",
      "\n",
      "First 5 rows of the scaled training set:\n",
      " [[-0.01  0.21 -0.12  0.01  0.31 -0.02 -0.43 -0.3   1.46 -0.68 -0.76]\n",
      " [-0.8  -0.85 -0.94 -0.96 -0.92 -0.91 -0.97 -0.92 -0.68  1.47 -0.76]\n",
      " [-0.59 -0.6  -0.83 -0.85 -0.9  -0.78 -0.69  0.31 -0.68 -0.68  1.32]\n",
      " [-2.68 -2.57 -2.24 -1.61 -1.55 -1.62 -1.61 -0.92 -0.68 -0.68  1.32]\n",
      " [ 0.53  0.56  0.47  0.54  0.64  0.64  0.38  0.93  1.46 -0.68 -0.76]]\n",
      "\n",
      "First 5 rows of the scaled testing set:\n",
      " [[-2.6  -2.47 -2.   -1.58 -1.51 -1.55 -1.59 -1.54 -0.68  1.47 -0.76]\n",
      " [ 0.66  0.51  0.7   0.58 -0.07  1.12  0.52  0.93  1.46 -0.68 -0.76]\n",
      " [ 1.37  1.32  0.35  1.26  1.17  1.17  1.06 -0.3   1.46 -0.68 -0.76]\n",
      " [ 1.03  0.91  0.7   1.15  1.48  1.24  0.58  0.31  1.46 -0.68 -0.76]\n",
      " [ 0.24  0.16 -0.    0.26  0.34  0.38 -0.01 -0.92  1.46 -0.68 -0.76]]\n",
      "\n",
      "First 5 rows of the scaled validation set:\n",
      " [[-9.28e-01 -1.01e+00 -1.18e+00 -1.04e+00 -1.04e+00 -9.96e-01 -8.99e-01\n",
      "  -6.13e-01 -6.83e-01 -6.79e-01  1.32e+00]\n",
      " [ 4.50e-01  4.59e-01 -4.74e-01  2.11e-01  1.61e-01  6.84e-01  1.69e-01\n",
      "   3.14e-01  1.46e+00 -6.79e-01 -7.60e-01]\n",
      " [ 9.51e-01  1.17e+00  1.76e+00  1.92e+00  4.05e-01  9.45e-01  2.20e+00\n",
      "   3.71e+00  1.46e+00 -6.79e-01 -7.60e-01]\n",
      " [-1.10e+00 -1.01e+00 -1.06e+00 -1.06e+00 -1.04e+00 -1.01e+00 -1.06e+00\n",
      "  -9.21e-01 -6.83e-01  1.47e+00 -7.60e-01]\n",
      " [ 3.22e-02  1.06e-01 -3.02e-03 -3.25e-01 -4.79e-01  2.94e-02 -2.37e-01\n",
      "  -3.04e-01 -6.83e-01  1.47e+00 -7.60e-01]]\n",
      "Training Set: 2923 samples\n",
      "Testing Set: 627 samples\n",
      "Validation Set: 627 samples\n"
     ]
    }
   ],
   "source": [
    "%run DataSplitting.ipynb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "# from public_tests import *\n",
    "# from autils import *\n",
    "# from lab_utils_softmax import plt_softmax\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):\n",
    "    ### START CODE HERE ###\n",
    "    ez = np.exp(z)\n",
    "    a = ez/np.sum(ez)\n",
    "    ### END CODE HERE ###\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_random_seed(1234) # for consistent results\n",
    "model = Sequential(\n",
    "    [\n",
    "        ### START CODE HERE ###\n",
    "        tf.keras.layers.InputLayer((11,)),\n",
    "        tf.keras.layers.Dense(5, activation=\"relu\", name=\"L1\"),\n",
    "        tf.keras.layers.Dense(3, activation=\"linear\", name=\"L2\")\n",
    "        ### END CODE HERE ###\n",
    "    ], name = \"my_model\"\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "L1 (Dense)                   (None, 5)                 60        \n",
      "_________________________________________________________________\n",
      "L2 (Dense)                   (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 78\n",
      "Trainable params: 78\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Layer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape = (11, 5), b1 shape = (5,)\n",
      "W2 shape = (5, 3), b2 shape = (3,)\n"
     ]
    }
   ],
   "source": [
    "[layer1, layer2] = model.layers\n",
    "#### Examine Weights shapes\n",
    "W1,b1 = layer1.get_weights()\n",
    "W2,b2 = layer2.get_weights()\n",
    "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
    "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2923/2923 [==============================] - 0s 144us/sample - loss: 0.9467\n",
      "Epoch 2/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.6726\n",
      "Epoch 3/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.5962\n",
      "Epoch 4/40\n",
      "2923/2923 [==============================] - 0s 15us/sample - loss: 0.5545\n",
      "Epoch 5/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.5202\n",
      "Epoch 6/40\n",
      "2923/2923 [==============================] - 0s 17us/sample - loss: 0.4885\n",
      "Epoch 7/40\n",
      "2923/2923 [==============================] - 0s 17us/sample - loss: 0.4571\n",
      "Epoch 8/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.4257\n",
      "Epoch 9/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.3923\n",
      "Epoch 10/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.3607\n",
      "Epoch 11/40\n",
      "2923/2923 [==============================] - 0s 17us/sample - loss: 0.3331\n",
      "Epoch 12/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.3088\n",
      "Epoch 13/40\n",
      "2923/2923 [==============================] - 0s 18us/sample - loss: 0.2874\n",
      "Epoch 14/40\n",
      "2923/2923 [==============================] - 0s 21us/sample - loss: 0.2680\n",
      "Epoch 15/40\n",
      "2923/2923 [==============================] - 0s 24us/sample - loss: 0.2507\n",
      "Epoch 16/40\n",
      "2923/2923 [==============================] - 0s 22us/sample - loss: 0.2353\n",
      "Epoch 17/40\n",
      "2923/2923 [==============================] - 0s 22us/sample - loss: 0.2214\n",
      "Epoch 18/40\n",
      "2923/2923 [==============================] - 0s 23us/sample - loss: 0.2081\n",
      "Epoch 19/40\n",
      "2923/2923 [==============================] - 0s 22us/sample - loss: 0.1962\n",
      "Epoch 20/40\n",
      "2923/2923 [==============================] - 0s 20us/sample - loss: 0.1853\n",
      "Epoch 21/40\n",
      "2923/2923 [==============================] - 0s 20us/sample - loss: 0.1744\n",
      "Epoch 22/40\n",
      "2923/2923 [==============================] - 0s 19us/sample - loss: 0.1645\n",
      "Epoch 23/40\n",
      "2923/2923 [==============================] - 0s 20us/sample - loss: 0.1553\n",
      "Epoch 24/40\n",
      "2923/2923 [==============================] - 0s 18us/sample - loss: 0.1467\n",
      "Epoch 25/40\n",
      "2923/2923 [==============================] - 0s 19us/sample - loss: 0.1384\n",
      "Epoch 26/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.1304\n",
      "Epoch 27/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.1232\n",
      "Epoch 28/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.1163\n",
      "Epoch 29/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.1099\n",
      "Epoch 30/40\n",
      "2923/2923 [==============================] - 0s 15us/sample - loss: 0.1041\n",
      "Epoch 31/40\n",
      "2923/2923 [==============================] - 0s 15us/sample - loss: 0.0988\n",
      "Epoch 32/40\n",
      "2923/2923 [==============================] - 0s 15us/sample - loss: 0.0936\n",
      "Epoch 33/40\n",
      "2923/2923 [==============================] - 0s 15us/sample - loss: 0.0889\n",
      "Epoch 34/40\n",
      "2923/2923 [==============================] - 0s 15us/sample - loss: 0.0844\n",
      "Epoch 35/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.0805\n",
      "Epoch 36/40\n",
      "2923/2923 [==============================] - 0s 16us/sample - loss: 0.0765\n",
      "Epoch 37/40\n",
      "2923/2923 [==============================] - 0s 19us/sample - loss: 0.0728\n",
      "Epoch 38/40\n",
      "2923/2923 [==============================] - 0s 19us/sample - loss: 0.0697\n",
      "Epoch 39/40\n",
      "2923/2923 [==============================] - 0s 19us/sample - loss: 0.0663\n",
      "Epoch 40/40\n",
      "2923/2923 [==============================] - 0s 19us/sample - loss: 0.0631\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    ")\n",
    "\n",
    "#y_train = [Age<8, Age>=8 && Age<15, Age>15]\n",
    "#y_train_classification=['young','middle-age','old']\n",
    "#y_train_encoded = [2,0,1]\n",
    "history = model.fit(\n",
    "    X_train_scaled,y_train_encoded,\n",
    "    epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicting a Two: \n",
      "[[ -3.01 -10.82   6.5 ]]\n",
      " Largest Prediction index: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#y_train = [Age<8, Age>=8 && Age<15, Age>15]\n",
    "#y_train_classification=['young','middle-age','old']\n",
    "#y_train_encoded = [2,0,1]\n",
    "\n",
    "age_of_342 = X_train_scaled[123]\n",
    "prediction = model.predict(age_of_342.reshape(1,11))  # prediction\n",
    "print(f\" predicting a Two: \\n{prediction}\")\n",
    "print(f\" Largest Prediction index: {np.argmax(prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of output with SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting a Two. Probability vector:\n",
      "[[7.44e-05 3.00e-08 1.00e+00]]\n",
      "Total of predictions: 1.000\n",
      "np.argmax(prediction_p_np): 2\n"
     ]
    }
   ],
   "source": [
    "prediction_p = tf.nn.softmax(prediction)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    prediction_p_np = prediction_p.eval()\n",
    "\n",
    "print(f\"Predicting a Two. Probability vector:\\n{prediction_p_np}\")\n",
    "print(f\"Total of predictions: {np.sum(prediction_p_np):0.3f}\")\n",
    "\n",
    "yhat = np.argmax(prediction_p_np)\n",
    "print(f\"np.argmax(prediction_p_np): {yhat}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eltajenv",
   "language": "python",
   "name": "eltajenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
