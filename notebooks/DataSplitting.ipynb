{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551a4fc6",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Splitting for Abalone Dataset\n",
    "\n",
    "This notebook covers the steps for preprocessing the abalone dataset and splitting it into training, testing, and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2485511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "abalone_data_path = '../abalone/abalone.data'\n",
    "column_names = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\n",
    "abalone_df = pd.read_csv(abalone_data_path, header=None, names=column_names)\n",
    "abalone_df['Age'] = abalone_df['Rings'] + 1.5  # Age calculated from Rings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf397b3",
   "metadata": {},
   "source": [
    "## One-Hot Encoding the 'Sex' Column\n",
    "\n",
    "The 'Sex' column is a categorical feature and needs to be converted into a numerical format for machine learning algorithms. We use one-hot encoding to achieve this, resulting in separate columns for each category with binary values (0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ff1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the 'Sex' column\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "sex_encoded = encoder.fit_transform(abalone_df[['Sex']])\n",
    "sex_encoded_df = pd.DataFrame(sex_encoded, columns=encoder.get_feature_names_out(['Sex']))\n",
    "\n",
    "# Drop the original 'Sex' column and concatenate the encoded one\n",
    "abalone_df = abalone_df.drop('Sex', axis=1)\n",
    "abalone_df = pd.concat([abalone_df, sex_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895aa80b",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Features and Target\n",
    "\n",
    "We separate the dataset into features (`X`) and the target variable (`y`). The target variable in our case is 'Rings', which indicates the age of the abalone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30b3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X = abalone_df.drop('Rings', axis=1)\n",
    "y = abalone_df['Rings']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee39a7c",
   "metadata": {},
   "source": [
    "## Splitting the Data into Training, Testing, and Validation Sets\n",
    "\n",
    "We split the data into three parts:\n",
    "1. Training set (80% of the data): Used to train the model.\n",
    "2. Testing set (10% of the data): Used to test the model's performance after training.\n",
    "3. Validation set (10% of the data): Used to fine-tune the model's hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed409cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training (80%), and a temporary set (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the temporary set into testing and validation sets (50% each of the temporary set, 10% each of the total dataset)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a104247",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "We scale the features to ensure they contribute equally to the model's performance. This is particularly important for models that are sensitive to the scale of the input data, such as linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4950bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69a05b",
   "metadata": {},
   "source": [
    "## Verifying the Sizes of the Splits\n",
    "\n",
    "We print the sizes of each split to ensure the data has been divided correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89af784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 3341 samples\n",
      "Testing Set: 418 samples\n",
      "Validation Set: 418 samples\n"
     ]
    }
   ],
   "source": [
    "# Verifying the sizes of the splits\n",
    "print(f\"Training Set: {len(X_train)} samples\")\n",
    "print(f\"Testing Set: {len(X_test)} samples\")\n",
    "print(f\"Validation Set: {len(X_val)} samples\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
